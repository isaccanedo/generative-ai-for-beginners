# 生成式AI初学者指南：第13章 - 保护AI应用

[![保护您的AI应用](../../images/13-lesson-banner.jpg?WT.mc_id=academic-105485-koreyst)]()

## 引言

本课程包括：

- AI系统的安全问题。
- AI系统面临的常见风险与威胁。
- 保障AI系统安全的方法与策略。

## 学习目标

学习完本课程，你将能够：

- 理解AI系统面临的风险与威胁。
- 掌握保护AI系统的常用方法与最佳实践。
- 了解如何通过安全测试预防意外事件和维护用户信任。

## 生成式AI中的安全是什么意思？

随着人工智能（AI）和机器学习（ML）技术越来越多地影响我们的生活，保护客户数据和AI系统本身变得尤为重要。AI/ML在决策支持方面的应用越来越广泛，一旦决策出错，可能会带来严重的后果。

需要关注的关键点包括：

- **AI/ML的影响**：AI/ML对我们生活的深远影响使得保护它们变得非常重要。
- **安全挑战**：我们需要重视AI/ML带来的影响，以防止AI产品遭受复杂攻击，无论是来自恶意用户还是有组织的团体。
- **战略性问题**：科技产业需要主动应对战略挑战，保证客户安全和数据安全。

此外，机器学习模型往往无法区分恶意输入和异常数据。大量训练数据来自于未经筛选、未经审查的公开数据集，任何第三方都可以贡献内容。攻击者可以轻松贡献数据而不必直接篡改数据集。随着时间的推移，如果数据格式正确，即便是低可信度的恶意数据也可能变成高可信度的数据。

因此，确保模型决策依赖的数据存储的完整性和安全至关重要。

## 理解AI的威胁和风险

目前，数据污染是AI及相关系统面临的最大安全威胁。数据污染是指有意修改训练AI所用的数据，从而导致AI犯错。这种情况之所以发生，一方面是因为缺乏标准化的检测和缓解方法，另一方面是因为我们依赖于不可靠或未经筛选的公共数据集进行训练。跟踪数据的来源和血统，以维护数据完整性并防止训练过程出错，显得尤为重要。否则，就会出现“垃圾进，垃圾出”的情况，最终影响模型的性能。

数据污染对模型的影响示例包括：

1. **标签翻转**：在二分类任务中，攻击者故意更改少数训练数据的标签，如将良性样本标记为恶意，导致模型学习到错误的关联。.\
   **示例**：因标签被篡改，导致垃圾邮件过滤器将合法邮件错误分类为垃圾邮件。
2. **特征污染**：攻击者微调训练数据中的特征，引入偏见或误导模型。.\
   **示例**：在产品描述中添加无关关键词，以操纵推荐系统。
3. **数据注入**：向训练集注入恶意数据，影响模型行为。.\
   **示例**：添加假用户评论，歪曲情感分析结果。
4. **后门攻击**：攻击者在训练数据中插入隐藏模式（后门），使模型在触发时表现出恶意行为。.\
   **示例**：通过训练含有后门的图像，使面部识别系统错误识别特定人物。

MITRE公司创建了[ATLAS(Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)，一个记录对手在真实攻击AI系统中使用的策略和技术的知识库。

> 随着AI技术的融合，AI系统的漏洞数量增加，这超出了传统网络攻击的范畴。我们开发ATLAS的目的是提高对这些独特且不断演变的漏洞的认识，因为全球社区越来越多地将AI纳入各种系统中。ATLAS参照MITRE ATT&CK®框架建立，其策略、技术和程序（TTPs）与ATT&CK中的内容相辅相成。

与MITRE ATT&CK®框架一样——该框架广泛用于传统网络安全，规划高级威胁模拟场景——ATLAS提供了一套易于搜索的TTPs，帮助我们更好地理解和准备防御新兴攻击。

Open Web Application Security Project (OWASP)还创建了一个“[前十大漏洞列表](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)”，列出了利用大型语言模型(LLMs)的应用程序中最关键的漏洞。列表强调了包括数据污染在内的威胁风险，以及其他风险，例如：

- **提示注入**：通过精心设计的输入操纵大型语言模型(LLM)，使其偏离预期行为的技术。
- **供应链漏洞**：构成LLM应用程序的组件和软件可能被威胁，导致意外结果、引入偏见，甚至是基础设施漏洞。
- **过度依赖**：LLMs容易出错，可能提供不准确或不安全的结果。在多个实例中，人们将结果视为准确无误，导致了意想不到的负面后果。

Microsoft Cloud Advocate Rod Trent撰写的免费电子书[必须学习的AI安全](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst)深入探讨了这些和其他新兴AI威胁，并提供了广泛的指导，以应对这些挑战。

## AI系统和大型语言模型的安全测试

人工智能（AI）正在改变各种领域和行业，为社会带来新的可能性和好处。然而，AI也带来了数据隐私、偏见、缺乏可解释性和潜在滥用等重大挑战和风险。因此，确保AI系统的安全性和责任感至关重要，这意味着它们需要遵守道德和法律标准，并获得用户和利益相关者的信任。

安全测试是通过识别和利用它们的漏洞来评估AI系统或大型语言模型的安全性的过程。这项测试可以由开发者、用户或第三方审计员根据测试的目的和范围进行。AI系统和大型语言模型的一些常见安全测试方法包括：

- **数据清洗**：从AI系统或大型语言模型的训练数据或输入中删除或匿名化敏感或私有信息的过程。数据清洗有助于防止数据泄露和恶意操作，通过减少机密或个人数据的暴露。
- **对抗性测试**：生成并应用对抗性示例到AI系统或大型语言模型的输入或输出的过程，以评估其对对抗性攻击的鲁棒性和恢复力。对抗性测试有助于识别和缓解AI系统或大型语言模型可能被攻击者利用的漏洞和弱点。
- **模型验证**：验证AI系统或大型语言模型的模型参数或架构的正确性和完整性的过程。模型验证有助于检测和防止模型盗用，确保模型受到保护和认证。
- **输出验证**：验证AI系统或大型语言模型输出的质量和可靠性的过程。输出验证有助于检测和纠正恶意操作，确保输出是一致和准确的。

OpenAI作为AI系统的领先者，建立了一系列_安全评估_作为其红队网络计划的一部分，旨在测试AI系统的输出，希望为AI安全做出贡献。

> 评估可以从简单的问答测试到更复杂的模拟。以下是一些OpenAI开发的样本评估，用于从多个角度评估AI行为的具体示例：

#### 说服(Persuasion)

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst)：一个AI系统能多好地欺骗另一个AI系统说出一个秘密词？
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst)：一个AI系统能多好地说服另一个AI系统捐款？
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst)：一个AI系统能多好地影响另一个AI系统对政治提案的支持？

#### 隐写术（隐藏信息）

- [Steganography](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst)：一个AI系统能多好地传递秘密消息而不被另一个AI系统发现？
- [Text Compression](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst)：一个AI系统在压缩和解压消息方面的表现如何，以便隐藏秘密消息？
- [Schelling Point](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst)：一个AI系统在没有直接通信的情况下与另一个AI系统协调的能力如何？

### AI安全

我们的目标是保护AI系统免受恶意攻击、滥用或意外后果的影响。这包括采取措施确保AI系统的安全、可靠性和可信度，例如：

- 保护用于训练和运行AI模型的数据和算法
- 防止未经授权的访问、操纵或破坏AI系统
- 检测和缓解AI系统中的偏见、歧视或道德问题
- 确保AI决策和行动的问责性、透明度和可解释性
- 使AI系统的目标和价值与人类和社会的目标和价值一致

AI安全对于确保AI系统和数据的完整性、可用性和机密性至关重要。AI安全的挑战和机遇包括：

- 机遇：将AI纳入网络安全策略，因为它可以在识别威胁和提高响应时间方面发挥关键作用。AI可以帮助自动化和增强检测和缓解网络攻击的能力，如网络钓鱼、恶意软件或勒索软件。
- 挑战：对手也可以使用AI发起复杂的攻击，如生成假或误导性内容、冒充用户或利用AI系统中的漏洞。因此，AI开发者有独特的责任设计出对滥用具有韧性和抵抗力的系统。

### 数据保护

LLMs可能对它们使用的数据的隐私和安全构成风险。例如，LLMs可能会记住并泄露其训练数据中的敏感信息，如个人姓名、地址、密码或信用卡号码。它们还可能被恶意行为者操纵或攻击，这些人想要利用它们的漏洞或偏见。因此，了解这些风险并采取适当措施保护LLMs使用的数据很重要。您可以采取几个步骤来保护与LLMs一起使用的数据。这些步骤包括：

- **限制他们与LLMs共享的数据的数量和类型**：只共享对预期目的必要和相关的数据，并避免共享任何敏感、机密或个人的数据。用户还应该匿名化或加密他们与LLMs共享的数据，例如通过删除或掩盖任何识别信息，或使用安全的通信渠道。
- **验证LLMs生成的数据**：始终检查LLMs生成的输出的准确性和质量，以确保它们不包含任何不需要或不适当的信息。
- **报告和警告任何数据泄露或事件**：警惕LLMs产生的任何可疑或异常的活动或行为，例如生成不相关、不准确、冒犯性或有害的文本。这可能是数据泄露或安全事件的迹象。

数据安全、治理和合规对于任何希望在多云环境中利用数据和AI的力量的组织至关重要。保护和治理您的所有数据是一项复杂和多方面的任务。您需要在多个云中的不同位置保护和治理不同类型的数据（结构化、非结构化和由AI生成的数据），并需要考虑现有和未来的数据安全、治理和AI法规。为了保护您的数据，您需要采取一些最佳实践和预防措施，例如：

- 使用提供数据保护和隐私功能的云服务或平台。
- 使用数据质量和验证工具检查您的数据是否有错误、不一致或异常。
- 使用数据治理和伦理框架确保您的数据以负责任和透明的方式使用。

### 模拟现实世界威胁 - AI红队(AI red teaming)

模拟现实世界威胁现在被认为是构建弹性AI系统的标准实践，通过采用类似的工具、策略、程序来识别系统的风险并测试防御者的响应。

> AI红队的实践已经演变为具有更广泛的含义：它不仅涵盖了探测安全漏洞，还包括探测其他系统故障，如生成可能有害的内容。AI系统带来了新的风险，而红队是理解这些新风险的核心，如提示注入和产生不切实际的内容。- [Microsoft AI红队构建更安全的AI未来](https://www.microsoft.com/en-us/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![红队指引和资源](../../images/13-AI-red-team.png?WT.mc_id=academic-105485-koreyst)]()

以下是塑造Microsoft AI红队计划的关键洞察。

1. **AI红队的广泛范围：**AI红队现在包括安全和负责任AI（RAI）结果。传统上，红队专注于安全方面，将模型视为向量（例如，盗用底层模型）。然而，AI系统引入了新的安全漏洞（例如，提示注入，污染），需要特别关注。除了安全之外，AI红队还探测公平问题（例如，刻板印象）和有害内容（例如，暴力美化）。及早识别这些问题允许优先考虑防御投资。
2. **恶意和良性失败：**AI红队考虑了恶意和良性视角的失败。例如，当红队新的Bing时，我们探索的不仅仅是恶意对手如何颠覆系统，还有普通用户可能遇到的问题或有害内容。与传统的安全红队不同，主要关注恶意行为者，AI红队考虑了更广泛的人物和潜在失败。
3. **AI系统的动态性：**AI应用不断进化。在大语言模型应用中，开发者适应不断变化的要求。持续的红队确保持续的警惕和适应不断变化的风险。

AI红队并非包罗万象，应被视为补充其他控制措施，如[基于角色的访问控制（RBAC）](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst)和全面的数据管理解决方案。它旨在补充一个安全策略，该策略侧重于采用安全和负责任的AI解决方案，这些解决方案考虑到隐私和安全，同时努力最小化偏见、有害内容和误导信息，这些都可能侵蚀用户信心。

以下是一些可以帮助您更好地了解红队如何帮助识别和缓解AI系统中的风险的额外阅读材料：

- [为大型语言模型（LLMs）及其应用规划红队](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [什么是OpenAI红队网络？](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI红队 - 构建更安全、更负责任的AI解决方案的关键实践](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)，一个知识库，记录了对手在对AI系统进行实际攻击中使用的策略和技术。

## 知识检查

维护数据完整性和防止滥用的有效方法是什么？

1. 为数据访问和数据管理实施强大的基于角色的控制
2. 执行并审计数据标签，以防止数据误表达或滥用
3. 确保您的AI基础设施支持内容过滤

答案：1，虽然所有这三个建议都很有价值，但确保正确分配用户数据访问权限将大大减少通过LLMs操纵和误解数据的风险。

## 🚀 挑战

探索更多关于如何在AI时代[管理和保护敏感信息](https://learn.microsoft.com/en-us/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst)的方法。

## 做得好，继续你的学习之旅

完成本课程后，请继续探索我们的[生成式AI学习系列](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)，以进一步提高你的生成式AI技能！

下一站，第14课：[生成式AI应用生命周期](../../../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)！
