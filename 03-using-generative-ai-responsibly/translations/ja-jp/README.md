# 生成 AI の責任ある使用

[![生成 AI の責任ある使用](../../images/03-lesson-banner.png?WT.mc_id=academic-105485-yoterada)]() 

> **ビデオは近日公開予定**

AI、特に生成 AI にとても強い興味は抱くのは理解できますが、それをどのように責任を持って利用するかについて、十分に検討する必要があります。例えば、出力結果が「公平で有害でない」のを保証するなど、多くの点を考慮すべきです。本章は、上記の背景や考慮すべき点、そして AI をより有効活用するための具体的な行動指針を、皆様にお届けします。

## はじめに

このレッスンでは、下記の内容について説明します。

- 生成 AI アプリケーションを構築する際に、なぜ責任ある AI を重視すべきか
- 責任ある AI の基本原則と、それが生成 AI とどう結びつくのか
- 戦略とツールを使って、責任ある AI の原則をどのように実践するのか

## 学習目標  

このレッスンを修了すると、下記を理解できます：

- 生成 AI アプリケーション構築時の、責任ある AI の重要性
- 生成 AI アプリケーションの構築時、責任ある AI の基本原則をいつ、どのように検討し適用するか
- 責任ある AI の概念を実践するために利用可能なツールと戦略

## 責任ある AI の原則

生成 AI に対する興味は、今までにないほど高まっています。この盛り上がりは、新たな開発者や関心、資金をこの業界にもたらしています。これは、生成 AI を使用して新しい製品や、新しい企業を創業する人々にとって非常に好意的な状況ですが、同時に責任を持って進めるのも重要です。  

このコースでは、スタートアップが AI を利用した教育製品の実装に焦点を当てます。公平性、包括性、信頼性・安全性、セキュリティ・プライバシー、透明性、説明責任といった責任ある AI の原則に基づいて、これらが製品における生成 AI の活用と、どのように関連していくのかについて考えます。

## 責任ある AI を優先すべき理由

製品開発時、「利用者の利益を最優先に考える顧客中心のアプローチ」で進めると、最良の結果が得られます。

生成 AI は、利用者に役立つ回答、情報、ガイダンス、コンテンツを生成する力があります。こうした作業の多くは手作業を必要とせずに実行でき、それによって非常に素晴らしい成果を生み出せます。しかし、適切な計画と戦略がなければ、残念ながら利用者、製品、そして社会全体に悪影響をもたらす場合もあります。  

下記に記載する、潜在的に害を及ぼす可能性のある例（全てではありませんが）をご覧ください：

### Hallucinations (幻覚)

「Hallucinations (幻覚)」とは、LLM が完全に無意味な内容や、他の情報に基づいて事実と異なる内容を出力する際に使用する用語です。

例えば、スタートアップで、学生から AI モデルに対して歴史に関する問い合わせが可能な機能を作ったとします。そして、ある学生がこんな質問をしました  
`タイタニックの唯一の生き残りは誰ですか？`

すると、モデルは以下のような回答を生成します

> [!NOTE]
> 訳者追記  
> ご注意：毎回同じ回答結果ではありませんし、利用するモデルのバージョンによっても回答内容は異なります

![Prompt saying "Who was the sole survivor of the Titanic"](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-yoterada)

> [!TIP]
> 訳者追記  
> プロンプト：タイタニックの唯一の生き残りは誰ですか？  
> 回答：タイタニック号の事故の唯一の生存者は、チャールズ・ジョーギンでした。彼はイギリスのパン職人で、タイタニック号のチーフ・ベーカリー職人でした。ジョーギンは、転覆した救命ボートにしがみつき、極寒の海で数時間耐え、RMS カルパチアに救助されたと伝えられています。彼の生存は、彼が大量のアルコールを摂取していたため、極度の寒さに耐えられたのではないかと伝えられています。ジョーギンは 1956 年まで生存し、後にタイタニック号の事故の経験について証言しました。

> *(Source: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-yoterada))*

これはとても自信に満ちて、詳細な回答結果です。しかし残念ながら、この回答は間違っています。タイタニック号の生存者は複数名いたという事実が、調査結果から判明しています。一方で、この課題研究を始めたばかりの学生にとって、この回答は疑う余地のないほど、そして事実として受け入れられるほどの説得力があります。このような間違いのある回答結果から、学生は AI システムを信頼できなくなり、さらにはスタートアップ企業の評判にも悪影響を及ぼす可能性があります。

LLM のバージョンを更新するごとに、幻覚を減らす方向でパフォーマンス改善がはかられています。しかし、こうした改善があったとしても、アプリケーションの開発者や利用者は、こうした回答捏造の問題を意識する必要があります。

### 有害なコンテンツ

前のセクションで、LLM が不正確、または無意味な回答を出力する可能性について触れました。もう一つ注意すべきリスクは、モデルが有害なコンテンツを回答する場合です。  

有害なコンテンツには以下のようなものがあります：  

- 自傷行為や、特定グループへの危害を助長、または奨励する指示
- 憎悪や侮辱的な内容
- あらゆる種類の攻撃や、暴力行為の計画
- 違法なコンテンツを探す方法や、犯罪を犯す方法
- 性的に露骨な内容

スタートアップでは、このようなコンテンツが学生に見られないように、適切なツールと戦略を確実に導入したいと考えています。  

### 公平性の欠如

公平性とは、「AI システムがいかなる偏見や差別も抱かず、全人類に対して公平かつ平等にサービスを提供する状況」を指します。生成 AI の世界では、マイノリティ・グループの排他的な世界観が、モデルで再強化されないように注意しなければなりません。

このような公平性が欠如した出力は、利用者にとって好意的な製品体験を妨げるだけでなく、社会的な損害をさらに引き起こします。アプリケーション開発者は、生成 AI を活用したソリューションを作る際、多様な利用者層を考慮する必要があります。

## 責任ある生成 AI の使用方法

責任ある生成 AI の重要性を理解した上で、責任を持って AI ソリューションを構築するための、4 つの重点項目をご覧ください。

![Mitigate Cycle](../../images/mitigate-cycle.png?WT.mc_id=academic-105485-yoterada)

### 潜在的な危害の測定

一般的なソフトウェアのテストでは、アプリケーションに対して、利用者がおこなう行動を予想してテストを行います。同様に、利用者が入力する可能性のあるプロンプトを、様々なテスト・ケースを用いて実行すると、潜在的な問題を事前に発見できる可能性があります。

スタートアップが教育関連の製品を実装しているので、教育関連のプロンプトのリストを準備すると良いでしょう。これには、特定の科目、歴史的事実、学生生活に関するプロンプトなどが含まれるかもしれません。

### 潜在的な危害の軽減

AI モデルとその出力によって引き起こされる、潜在的な問題を防ぐ、または抑制する方法を検討する時が来ました。これを4つの異なるレイヤーで考えます。

![Mitigation Layers](../../images/mitigation-layers.png?WT.mc_id=academic-105485-yoterada)

- **AI モデル**：用途に適したモデルを選択します。GPT-4 のように大きくて複雑なモデルを、より小規模で特定の用途に使う場合、有害な内容を引き起こす可能性があり、リスクが高まります。独自のトレーニング・データを使ってモデルをファイン・チューニングし、有害コンテンツのリスクを減らせます。

- **安全システム**：安全システムとは、モデルを提供するプロバイダのプラットフォーム上で危害を軽減するために使用する設定やツールです。例えば、Azure OpenAI サービスのコンテンツ・フィルタリング・システムがあります。安全システムはボットにおける、脱獄攻撃 (Jailbreak) や、望ましくない活動などのリクエストも検出する必要があります。

- **メタ・プロンプト**：メタ・プロンプトやグラウンディングといった手法を使って、モデルの行動や情報を特定の方向に誘導したり制限できます。これはシステム入力を使って、モデルに対して特定の制限を定義します。また、システムのスコープや領域に合った、より適切な結果も出力できます。

> [!TIP]
> 訳者追記：  
> グラウンディングの詳細は、[こちらの記事](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/grounding-llms/ba-p/3843857?WT.mc_id=academic-105485-yoterada)をご参照ください。

信頼性のある情報源のデータだけをモデルが利用するように、Retrieval Augmented Generation（RAG）のような技術を使用できます。このコースの後半で、検索アプリケーションの構築に関するレッスンもあります。

- **ユーザー・エクスペリエンス**：最後のレイヤーは、ユーザ・インターフェースです。利用者はどのような形であれ、アプリケーションが提供するインターフェイスを利用してモデルとやりとりを行います。適切な UI/UX を設計し、モデルに送信する入力の種類や利用者で表示されるテキストや画像を制限できます。AI アプリケーションをデプロイする際、生成 AI アプリケーションでできる処理と、できない処理について透明性を持って利用者に伝える必要があります。

    本レッスンの最終章に「[12. AI アプリケーションの UX デザイン](../../../12-designing-ux-for-ai-applications/translations/ja-jp/README.md?WT.mc_id=academic-105485-yoterada)」に特化したレッスンがあります。

- **モデルを評価**：大規模言語モデル（LLM）は、そのモデルがどのようなデータを使って学習されたのかを完全に把握できないため、扱うのが難しい場合があります。それでもなお、モデルの性能や出力は常に検証すべきです。特に出力の正確性、類似性、根拠の正しさ、関連性の評価は重要です。これにより、利害関係者や利用者に透明性と信頼性を提供できます。

### 責任ある生成 AI ソリューションを運用する

    本番環境用の AI アプリケーションを構築する最終段階に入りました。スタートアップにおいては、法務部門やセキュリティ部門との協力を通じて、全ての規制方針に準拠しているかを確認する作業も含まれます。サービスのローンチ前には、デリバリー計画、問題発生時の対応方法、不具合が発生した際に元に戻すロールバック手順を作成し、利用者に被害が及ばないような戦略を練りたいと思います。

## ツール

責任ある AI ソリューションを開発する作業は、手間がかかるように感じられるかもしれません。しかし、その労力に見合う価値があります。生成 AI のビジネスが進化するに連れて、開発作業中に責任ある AI を効率的に取り入れられるツールが、より充実していくでしょう。例えば、[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-yoterada) は、API を呼び出す際に有害なコンテンツや画像を検出するのに役立ちます。

## 知識チェック

責任ある AI の利用を確実にするために、どのような点に考慮する必要があるのでしょうか？

1. 常に回答が正しい  
2. AI が悪用されたり犯罪目的で利用されないように注意する
3. AI が偏りや差別的な内容を含まないようにする

A: 2 と 3 が正解です。責任ある AI を導入し、害を及ぼす可能性のある影響や偏見をどのように緩和できるか、さらにその他のさまざまな問題について考える手助けにもなります。

## 🚀 Challenge

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-yoterada) について調べ、ご自身の用途に適用できるかご検討ください。  

## お疲れ様でした! 次のレッスンを続ける

このレッスン終了後、[生成 AI 学習コレクション](https://aka.ms/genai-collection?WT.mc_id=academic-105485-yoterada)をチェックして、生成 AI の知識をさらに深めましょう。  

レッスン 4 では、[プロンプト・エンジニアリングの基本](../../../04-prompt-engineering-fundamentals/translations/ja-jp/README.md?WT.mc_id=academic-105485-yoterada)について学びます！
